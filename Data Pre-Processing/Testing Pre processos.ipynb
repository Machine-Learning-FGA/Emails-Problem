{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processamento 20 new groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Time: 0.476s\n",
      "b\"From: mpretzel@cs.utexas.edu (Benjamin W. Allums)\\nSubject: Re: Mac II SCSI & PMMU socket question\\n\\nIn article <1qkmb2$n0d@jethro.Corp.Sun.COM> khc@marantz.Corp.Sun.COM writes:\\n\\n>1. The Mac II is supposed to have a socket for the MC68851 PMMU chip. Could\\n>anyone let me know where that socket is on the motherboard. I have obtained\\n>a PMMU chip (16 Mhz) from a surplus store, and would like to install it onto\\n>my Mac II (circa 1987). But I cannot see the socket myself when I tried to\\n>install it.\\n\\nThe original Mac II had an Apple MMU chip installed which performs a subset\\nof the 68851's functions.  If you look underneath your front left floppy\\nbay you will find three chips, all approximately the same size.  One will\\nbe the 68020, the next the 68881, and the third, approximately the same\\nsize, will be the Apple chip.  It is easy to spot because it has a 'hump'\\nin the middle of it.\\n\\n\\nExample:\\n\\n\\n                         -----------\\n                        /           \\\\\\n         ---------------             ---------------\\n         |                                         |\\n         |                                         |\\n\\nThat and the Apple logo should make it easy to find.\\n\\nBen\\nmpretzel@cs.utexas.edu\\n\" [2 0 2 ..., 1 2 2]\n",
      "2928 2928\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "t0 = time()\n",
    "dataset = load_files(container_path='../smallerdataset', shuffle=True)\n",
    "print('Import Time: %0.3fs' % (time() - t0))\n",
    "X, y = dataset.data, dataset.target\n",
    "print(X[0], y)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def pre_process_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # Retirar caracteres especiais e digitos\n",
    "    text = text.lower() # Tudo para caixa baixa\n",
    "    text = text.split() # Retirar espaÃ§os exessivos\n",
    "    text = ' '.join(text)\n",
    "    # print('\\tTexto limpo.\\n', text)\n",
    "\n",
    "    # Tokenizar\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    # print('\\tTransformando em tokens.\\n', tokens)\n",
    "\n",
    "    # Remover as stopword\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in set(stop)]\n",
    "    # print('\\tRetirando stopwords.\\n', tokens)\n",
    "    \n",
    "    # Tirar palavras menores que 2 caracteres\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    # print('\\tRetirando palavras menores que 2.\\n', tokens)\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # print('\\tLemmatizing.\\n', tokens)\n",
    "    \n",
    "    # Texto pre processado\n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTexto limpo.\n",
      " hello how are u my friend you should be fine by now shouldn t you it is raining cats and dogs outside\n",
      "\tTransformando em tokens.\n",
      " ['hello', 'how', 'are', 'u', 'my', 'friend', 'you', 'should', 'be', 'fine', 'by', 'now', 'shouldn', 't', 'you', 'it', 'is', 'raining', 'cats', 'and', 'dogs', 'outside']\n",
      "\tRetirando stopwords.\n",
      " ['hello', 'u', 'friend', 'fine', 'raining', 'cats', 'dogs', 'outside']\n",
      "\tRetirando palavras menores que 2.\n",
      " ['hello', 'friend', 'fine', 'raining', 'cats', 'dogs', 'outside']\n",
      "\tLemmatizing.\n",
      " ['hello', 'friend', 'fine', 'raining', 'cat', 'dog', 'outside']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello friend fine raining cat dog outside'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process_text(b'Hello! How are u, my friend? You should be fine by now, shouldn\\'t you? It is raining cats and dogs outside')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.60123610496521\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "X = [pre_process_text(doc) for doc in X]\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45781707763671875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_bow = CountVectorizer(max_features=35000)\n",
    "t0 = time()\n",
    "X_bow = vectorizer_bow.fit_transform(X)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4903137683868408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=1, max_df=0.7, ngram_range=(1,2), norm='l2', lowercase=False, stop_words='english', max_features=57000)\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(X)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2928, 57000)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2928, 1040000)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "hasher = HashingVectorizer(stop_words='english',\n",
    "                           alternate_sign=False,\n",
    "                           norm=None,\n",
    "                           binary=False,\n",
    "                           n_features=1040000\n",
    "                          )\n",
    "vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "X_hs = vectorizer.fit_transform(X)\n",
    "X_hs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "X_train_bw, X_test_bw, y_train_bw, y_test_bw = train_test_split(X_bow, y, test_size=0.20, random_state=42)\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X_tfidf, y, test_size=0.20, random_state=42)\n",
    "X_train_hs, X_test_hs, y_train_hs, y_test_hs = train_test_split(X_hs, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2342, 35000) (586, 35000) (2342,) (586,)\n",
      "(2342, 57000) (586, 57000) (2342,) (586,)\n",
      "(2342, 1040000) (586, 1040000) (2342,) (586,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bw.shape, X_test_bw.shape, y_train_bw.shape, y_test_bw.shape)\n",
    "print(X_train_tf.shape, X_test_tf.shape, y_train_tf.shape, y_test_tf.shape)\n",
    "print(X_train_hs.shape, X_test_hs.shape, y_train_hs.shape, y_test_hs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151  43  11]\n",
      " [  4 185  16]\n",
      " [  0   7 169]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.74      0.84       205\n",
      "          1       0.79      0.90      0.84       205\n",
      "          2       0.86      0.96      0.91       176\n",
      "\n",
      "avg / total       0.88      0.86      0.86       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_nb.fit(X_train_bw, y_train_bw)\n",
    "y_pred_nb_bw = clf_nb.predict(X_test_bw)\n",
    "print(confusion_matrix(y_test_bw, y_pred_nb_bw))\n",
    "print(classification_report(y_test_bw, y_pred_nb_bw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185  18   2]\n",
      " [ 13 181  11]\n",
      " [  4   8 164]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.90      0.91       205\n",
      "          1       0.87      0.88      0.88       205\n",
      "          2       0.93      0.93      0.93       176\n",
      "\n",
      "avg / total       0.90      0.90      0.90       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_nb.fit(X_train_tf, y_train_tf)\n",
    "y_pred_nb_tfidf = clf_nb.predict(X_test_tf)\n",
    "print(confusion_matrix(y_test_tf, y_pred_nb_tfidf))\n",
    "print(classification_report(y_test_tf, y_pred_nb_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[186  18   1]\n",
      " [ 14 181  10]\n",
      " [  5   6 165]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.91      0.91       205\n",
      "          1       0.88      0.88      0.88       205\n",
      "          2       0.94      0.94      0.94       176\n",
      "\n",
      "avg / total       0.91      0.91      0.91       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_nb.fit(X_train_hs, y_train_hs)\n",
    "y_pred_nb_hs = clf_nb.predict(X_test_hs)\n",
    "print(confusion_matrix(y_test_hs, y_pred_nb_hs))\n",
    "print(classification_report(y_test_hs, y_pred_nb_hs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier(alpha=.0001, max_iter=50, penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180  21   4]\n",
      " [ 23 168  14]\n",
      " [  8  19 149]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.88      0.87       205\n",
      "          1       0.81      0.82      0.81       205\n",
      "          2       0.89      0.85      0.87       176\n",
      "\n",
      "avg / total       0.85      0.85      0.85       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_sgd.fit(X_train_bw, y_train_bw)\n",
    "y_pred_sgd_bw = clf_sgd.predict(X_test_bw)\n",
    "print(confusion_matrix(y_test_bw, y_pred_sgd_bw))\n",
    "print(classification_report(y_test_bw, y_pred_sgd_bw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189  13   3]\n",
      " [ 11 179  15]\n",
      " [  3   8 165]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.92      0.93       205\n",
      "          1       0.90      0.87      0.88       205\n",
      "          2       0.90      0.94      0.92       176\n",
      "\n",
      "avg / total       0.91      0.91      0.91       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_sgd.fit(X_train_tf, y_train_tf)\n",
    "y_pred_sgd_tfidf = clf_sgd.predict(X_test_tf)\n",
    "print(confusion_matrix(y_test_tf, y_pred_sgd_tfidf))\n",
    "print(classification_report(y_test_tf, y_pred_sgd_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[188  15   2]\n",
      " [ 17 173  15]\n",
      " [  3   7 166]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.92      0.91       205\n",
      "          1       0.89      0.84      0.87       205\n",
      "          2       0.91      0.94      0.92       176\n",
      "\n",
      "avg / total       0.90      0.90      0.90       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_sgd.fit(X_train_hs, y_train_hs)\n",
    "y_pred_sgd_hs = clf_sgd.predict(X_test_hs)\n",
    "print(confusion_matrix(y_test_hs, y_pred_sgd_hs))\n",
    "print(classification_report(y_test_hs, y_pred_sgd_hs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC (SVM) - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_svm = LinearSVC(penalty='l2', dual=True, tol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[184  17   4]\n",
      " [ 15 169  21]\n",
      " [  8  10 158]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.90      0.89       205\n",
      "          1       0.86      0.82      0.84       205\n",
      "          2       0.86      0.90      0.88       176\n",
      "\n",
      "avg / total       0.87      0.87      0.87       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svm.fit(X_train_bw, y_train_bw)\n",
    "y_pred_svm_bw = clf_svm.predict(X_test_bw)\n",
    "print(confusion_matrix(y_test_bw, y_pred_svm_bw))\n",
    "print(classification_report(y_test_bw, y_pred_svm_bw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189  13   3]\n",
      " [ 12 178  15]\n",
      " [  3   7 166]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.92      0.92       205\n",
      "          1       0.90      0.87      0.88       205\n",
      "          2       0.90      0.94      0.92       176\n",
      "\n",
      "avg / total       0.91      0.91      0.91       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svm.fit(X_train_tf, y_train_tf)\n",
    "y_pred_svm_tfidf = clf_svm.predict(X_test_tf)\n",
    "print(confusion_matrix(y_test_tf, y_pred_svm_tfidf))\n",
    "print(classification_report(y_test_tf, y_pred_svm_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[189  13   3]\n",
      " [ 16 177  12]\n",
      " [  2   8 166]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.92      0.92       205\n",
      "          1       0.89      0.86      0.88       205\n",
      "          2       0.92      0.94      0.93       176\n",
      "\n",
      "avg / total       0.91      0.91      0.91       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svm.fit(X_train_hs, y_train_hs)\n",
    "y_pred_svm_hs = clf_svm.predict(X_test_hs)\n",
    "print(confusion_matrix(y_test_hs, y_pred_svm_hs))\n",
    "print(classification_report(y_test_hs, y_pred_svm_hs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive-Agressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "clf_pa = PassiveAggressiveClassifier(max_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[185  18   2]\n",
      " [ 16 169  20]\n",
      " [  6   8 162]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.90      0.90       205\n",
      "          1       0.87      0.82      0.85       205\n",
      "          2       0.88      0.92      0.90       176\n",
      "\n",
      "avg / total       0.88      0.88      0.88       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_pa.fit(X_train_bw, y_train_bw)\n",
    "y_pred_pa_bw = clf_pa.predict(X_test_bw)\n",
    "print(confusion_matrix(y_test_bw, y_pred_pa_bw))\n",
    "print(classification_report(y_test_bw, y_pred_pa_bw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[188  14   3]\n",
      " [ 11 179  15]\n",
      " [  3   7 166]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.92      0.92       205\n",
      "          1       0.90      0.87      0.88       205\n",
      "          2       0.90      0.94      0.92       176\n",
      "\n",
      "avg / total       0.91      0.91      0.91       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_pa.fit(X_train_tf, y_train_tf)\n",
    "y_pred_pa_tfidf = clf_pa.predict(X_test_tf)\n",
    "print(confusion_matrix(y_test_tf, y_pred_pa_tfidf))\n",
    "print(classification_report(y_test_tf, y_pred_pa_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190  14   1]\n",
      " [ 17 174  14]\n",
      " [  4   7 165]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.93      0.91       205\n",
      "          1       0.89      0.85      0.87       205\n",
      "          2       0.92      0.94      0.93       176\n",
      "\n",
      "avg / total       0.90      0.90      0.90       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_pa.fit(X_train_hs, y_train_hs)\n",
    "y_pred_pa_hs = clf_pa.predict(X_test_hs)\n",
    "print(confusion_matrix(y_test_hs, y_pred_pa_hs))\n",
    "print(classification_report(y_test_hs, y_pred_pa_hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
