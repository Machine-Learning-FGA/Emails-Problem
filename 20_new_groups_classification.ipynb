{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação _20 New Groups_\n",
    "\n",
    "Primeiramente precisamos definir quais são os nosso dados. \n",
    "Os dados são uma coleção de aproximadamente 20000 documentos, particionados quase que igualmente entre 20 diferentes classes. Esses documentos são basicamente textos (linguagem natural) de emails correspondente à certos tópicos (sua classe).\n",
    "\n",
    "Os 20 grupos distintos de documentos são: \n",
    "\n",
    "|Classe||\n",
    "|:----:|:---:|\n",
    "|comp.graphics|comp.os.ms-windows.misc|\n",
    "|comp.sys.ibm.pc.hardware|comp.sys.mac.hardware|\n",
    "|comp.windows.x|\n",
    "|rec.autos|rec.motorcycles|\n",
    "|rec.sport.baseball|rec.sport.hockey|\t\n",
    "|sci.crypt|sci.electronics|\n",
    "|sci.med|sci.space|\n",
    "|misc.forsale|\n",
    "|talk.politics.misc|talk.politics.guns|\n",
    "|talk.politics.mideast|\ttalk.religion.misc|\n",
    "|alt.atheism|soc.religion.christian|\n",
    "\n",
    "Vale resaltar que os documentos estão organizados em diretórios, cada qual nomeado correspondentemente com sua classe.\n",
    "\n",
    "[Download do conjunto de dados 18828\\*](http://qwone.com/~jason/20Newsgroups/20news-18828.tar.gz)       \n",
    "[Referência dos dados](http://qwone.com/~jason/20Newsgroups/)\n",
    "\n",
    "_* Os dados utilizados serão referentes ao conjunto 18828, que remove duplicatas e cabeçalhos desnecessários._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação dos dados\n",
    "Para a importação dos dados, vamos utilizar a biblioteca do _scikit-learn_, [**load_files**](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html). Essa função utiliza dos nomes das sub-pastas, que estão contidas no diretório alvo para cadastrar as múltiplas classes, sendo assim, ao final da importação teremos tanto nosso vetor X quanto nosso vetor y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Time: 3.462s\n",
      "X size: 18828\n",
      "y size: 18828\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "t0 = time()\n",
    "dataset = load_files(container_path='dataset', shuffle=True)\n",
    "print('Import Time: %0.3fs' % (time() - t0))\n",
    "X, y = dataset.data, dataset.target\n",
    "print('X size: %d\\ny size: %d' % (len(X), len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processamento dos dados\n",
    "Após a importação dos nossos vetores, podemos partir para o tratamento dos nossos dados.   \n",
    "Como estamos trabalhando com textos, precisamos transformar isso em algo compreensível para o computador e para nossos algoritmos de classificação.   \n",
    "Dessa forma, precisamos transformar os dados textuais em números, ou melhor uma matriz numérica. Para tanto ulizaremos de uma gama de técnicas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biblioteca de processamento de linguagem natural\n",
    "Para executar algumas tarefas de processamento de linguagem natural iremos utilizar da biblioteca [**NLTK**](http://www.nltk.org), _Natural Language Toolkit_.   \n",
    "\n",
    "#### _Stop Words_\n",
    "Essa biblioteca nos auxiliará a realizar tarefas como a retirada de palavras de parada (ou palavras vazias), que podem ser consideradas irrelevantes ao contexto da fala e escrita. Podemos dizer que essas palavras são as mais comuns da língua ou palavras funcionais (_function words_), porém não existe uma lista universal de _stop words_, sendo que essas variam de contexto para contexto.   \n",
    "Para o nosso contexto, a lista disponível na biblioteca NLTK será o suficiente para o processamento inicial.\n",
    "\n",
    "#### Tokenização de palavras\n",
    "Também é necessário transformar nosso texto em '_tokens_', que é nosso documento quebrado em palavras, ou _tokens_. _Token_ é um pedaço do todo, então uma palavra é um _token_ em uma frase, e uma frase é um _token_ em um parágrafo. Para nosso problema transfomaremos nosso documento (documento sendo um conjunto de frases ou paragrafos) em palavras, formando um vetor.\n",
    "\n",
    "#### _Stemming_\n",
    "_Stemming_ é o processo de chegar a raiz de uma palavra, via cortes. Sendo assim, com o uso de regras básicas, qualquer _token_ pode ser lapidado à sua raiz, dessa forma, _stemming_ é um processo baseado em regras no qual queremos juntar toda variação diferente do _token_.\n",
    "\n",
    "#### _Lemmatization_\n",
    "_Lemmatization_ é uma maneira metódica de converter toda forma gramatical ou inflacionada de um termo à sua raiz. _Lemmatization_ usa o contexto e parte de fala (_part of speech_) para determinar a forma inflacionada de uma palavra e aplica diferentes regras de normalização para cada parte de fala para alcançar a forma raiz do temo (_lemma_).\n",
    "\n",
    "Para biblioteca NLTK é utilizado a base de dados _WordNet_ para atingir a raiz de cada _token_.\n",
    "\n",
    "##### Spacy (Outra biblioteca de NLP)\n",
    "Vale a pena citarmos outra biblioteca de processamento de dados de linguagem natural, a [**Spacy**](https://spacy.io/docs/usage/), que assim como a NLTK se especializa em tratamento de dados textuais.   \n",
    "Apesar dessa biblioteca ter uma usabilidade muito boa (não é necessario vários _imports_ ou linhas de código para atingir um objetivo), optamos por utilizar a NLTK pela sua maior base na comunidade (16 anos no mercado) e seu tempo de execução para métodos como _Lemmatization_, que é muito menor que o do **Spacy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk # Biblioteca de processamento de linguagem natural\n",
    "# Execute essa linha abaixo caso ocorra um erro de dependência\n",
    "# nltk.download('punkt') # Necessário para tokenizar (sent_tokenize)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def pre_process_text(text, lemmatize):\n",
    "    if not isinstance(text, str):\n",
    "        text = text.decode('ISO-8859-1')\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text) # Retirar caracteres especiais e digitos\n",
    "    text = text.lower() # Tudo para caixa baixa\n",
    "    text = text.split() # Retirar espaços exessivos\n",
    "    text = ' '.join(text)\n",
    "    # print('\\tTexto limpo.\\n', text)\n",
    "\n",
    "    # Tokenizar\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    # print('\\tTransformando em tokens.\\n', tokens)\n",
    "\n",
    "    # Remover as stopword\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in set(stop)]\n",
    "    # print('\\tRetirando stopwords.\\n', tokens)\n",
    "    \n",
    "    # Tirar palavras menores que 3 caracteres\n",
    "    tokens = [token for token in tokens if len(token) >= 2]\n",
    "    # print('\\tRetirando palavras menores que 2.\\n', tokens)\n",
    "    \n",
    "\n",
    "    if lemmatize:\n",
    "        # Lemmatize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        # print('\\tLemmatizing.\\n', tokens)\n",
    "    else:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Texto pre processado\n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning time: 75.818s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "X = [pre_process_text(doc, True) for doc in X]\n",
    "print('Cleaning time: %0.3fs' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de Características (_Features Extraction_)\n",
    "Agora que temos nossos dados limpos, podemos transforma-los em uma representação numérica, ou melhor em uma matriz.\n",
    "\n",
    "#### _Bag of Words_ (Punhado de palavras)\n",
    "A forma mais comum de alcançar esse objetivo é por meio do _Bag of Words_ que \"aprende\" um vocabulário a partir do nosso _corpus_ (conjunto de documentos) então modela cada documento a partir a contagem da ocorrencia de cada palavra.   \n",
    "Dessa forma descartamos a maioria da estrutura de um texto, como paragrafos, frases e formatação, apenas contando quão frequente é uma palavra em cada documento. Sendo assim, essa técnica levou a imagem de um saco, ou punhado de palavras.   \n",
    "Essa técnica é implementada pela função **_CountVectorizer_** da biblioteca _sklearn_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words time: 9.151s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_bow = CountVectorizer(max_features=100000) # Retiramos cerca de 2000 features\n",
    "t0 = time()\n",
    "X_bow = vectorizer_bow.fit_transform(X)\n",
    "print('Bag of words time: %0.3fs' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "_TF-IDF_ é uma boa medida estatística para refletir a relevância de um termo para o documento em uma coleção de documentos. Esse método calcula um peso para cada termo com base em quão frequente ele em um documento, mas não em vários doumentos no _corpus_. Sendo assim, se uma palavra aparece muito frequentemente em um documento especifico, mas não em vários documentos, é provável que esse termo é importante para aquele contexto daquele documento.   \n",
    "Para calcular esse peso o algoritmo utiliza das fórmulas abaixo:\n",
    "$$ tf = \\frac{Número\\ de\\ vezes\\ que\\ um\\ termo\\ aparece\\ no\\ documento}{Total\\ de\\ termos\\ no \\ documento}$$\n",
    "\n",
    "$$df = \\frac{d(número\\ de\\ documentos\\ contendo\\ um\\ termo\\ específico)}{D (tamanho\\ do\\ corpus)}$$\n",
    "\n",
    "Normalmente D > d e $\\log\\frac{d}{D}$ resulta em um valor negativo. Portanto, tomamos o inverso dessa fração e essencialmente comprimimos a escala dos calores para que quantidades muito altas ou muito baixas sejam fácilmente comparáveis.\n",
    "\n",
    "$$ idf = \\log\\big(\\frac{Total\\ de\\ documentos}{Número\\ de\\ documentos\\ contendo\\ um\\ termo\\ específico}\\big)$$\n",
    "\n",
    "$$tfidf = \\text{tf} \\log\\big(\\frac{Número\\ total\\ de\\ documentos + 1}{Número\\ de\\ documentos\\ contendo\\ um\\ termo\\ específico + 1}\\big) + 1$$\n",
    "\n",
    "Para executar o _TF-IDF_ foi utilizada a função da biblioteca _sklearn_ [**_TfidfVectorizer_**](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer).\n",
    "\n",
    "##### Utilizando da função TfidfVectorizer\n",
    "Para a função da biblioteca do _sklearn_ existe a possibilidade de utilizarmos N-Grams, que é a sequencialização de tokens, formando novos padrões, onde o contexto pode gerar mais informação.   \n",
    "Com isso podemos ter um entendimento maior de alguns assuntos, sendo assim, essa funcionalidade foi utilizada no treinamento da nossa aplicação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF time: 14.490s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(min_df=1,\n",
    "                                   max_df=0.7, # Retiramos palavras com alta recorrência em documentos, como \"From\" e \"Subject\"\n",
    "                                   ngram_range=(1,2), # N-Gram\n",
    "                                   norm='l2',\n",
    "                                   lowercase=False, # Já transformamos tudo em caixa baixa, isso não é mais necessário\n",
    "                                   stop_words='english', # Para retirar mais possíveis stopwords\n",
    "                                   max_features=1340000 # Retiramos certa de 4000 features\n",
    "                                  )\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(X)\n",
    "print('TF-IDF time: %0.3fs' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashing + TFIDF\n",
    "Essa implementação utiliza de _hashs_ para achar o _token_ de uma palavra para a característica numérica. Com essa implementação, alguns benefícios são alcançados:\n",
    "- Utiliza menos memória para _datasets_ grandes, já que não é necessário armazenar um vocabulário em memória;\n",
    "- É mais rápido para fazer _pickle_ e _un-pickle_, já que dessa forma não se mantém nenhum estado a não ser o dos parâmetros do construtor;\n",
    "- Pode ser usado em um fluxo de dados ou _pipelines_ paralelas, já que nenhum estado é computado durante o _fit_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashing + TF-IDF time: 3.245s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "hasher = HashingVectorizer(stop_words='english', # Para retirar mais possíveis stopwords\n",
    "                           alternate_sign=False,\n",
    "                           norm=None,\n",
    "                           binary=False,\n",
    "                           n_features=1040000 # Retiramos certa de 5000 features\n",
    "                          )\n",
    "vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "t0 = time()\n",
    "X_hs = vectorizer.fit_transform(X)\n",
    "print('Hashing + TF-IDF time: %0.3fs' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar dados de treinamento e teste\n",
    "Agora que temos nossos dados processados númericamente podemos utilizar um algoritimo de classificação para treinar um modelo.   \n",
    "Utilizaremos 0.8 / 0.2 para o treinamento de nossos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "X_train_bw, X_test_bw, y_train_bw, y_test_bw = train_test_split(X_bow, y, test_size=0.20)\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(X_tfidf, y, test_size=0.20)\n",
    "X_train_hs, X_test_hs, y_train_hs, y_test_hs = train_test_split(X_hs, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento\n",
    "Para o treinamento de um classificador, chegamos aos seguintes algoritimos:\n",
    "- Naive Bayes Multinomial\n",
    "- Stochastic Gradient Descent Classifier\n",
    "- Support Vector Machine Linear\n",
    "- Passive Agressive Classifier\n",
    "\n",
    "Para mais informações sobre tais algoritimos refira-se a [página de conhecimento do grupo](https://github.com/Segmentation-Fault-Machine-Learning/Knowledge/wiki/An%C3%A1lise-dos-algoritmos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sgd = SGDClassifier(alpha=.0001, max_iter=50, penalty=\"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_svm = LinearSVC(penalty='l2', dual=True, tol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Passive Agressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "clf_pa = PassiveAggressiveClassifier(max_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escolha do Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BagOfWords\n",
      "\t NaiveBayesMultinomial - 0.8893273835151685\n",
      "\t SGDClassifier - 0.8771647302920214\n",
      "\t SVMLinear - 0.9015634925106856\n",
      "\t PassiveAgressive - 0.9018596049848584\n",
      "TF-IDF\n",
      "\t NaiveBayesMultinomial - 0.9104694585573714\n",
      "\t SGDClassifier - 0.9350964504055984\n",
      "\t SVMLinear - 0.9421406846014694\n",
      "\t PassiveAgressive - 0.9416056553492668\n",
      "Hashing+TFIDF\n",
      "\t NaiveBayesMultinomial - 0.8963245447055203\n",
      "\t SGDClassifier - 0.9226840317574634\n",
      "\t SVMLinear - 0.9329969907055279\n",
      "\t PassiveAgressive - 0.9283832737401811\n",
      "Execution time 1345.506s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "\n",
    "classifiers = {\n",
    "    'NaiveBayesMultinomial': clf_nb,\n",
    "    'SGDClassifier': clf_sgd,\n",
    "    'SVMLinear': clf_svm,\n",
    "    'PassiveAgressive': clf_pa\n",
    "}\n",
    "X_s = {\n",
    "    'BagOfWords': X_bow, \n",
    "    'TF-IDF': X_tfidf, \n",
    "    'Hashing+TFIDF': X_hs\n",
    "}\n",
    "scoring = {'prec_macro': 'precision_macro',\n",
    "           'rec_micro': make_scorer(recall_score, average='macro')}\n",
    "t0 = time()\n",
    "for X in X_s:\n",
    "    print (X)\n",
    "    for clf in classifiers:\n",
    "        scores = cross_validate(classifiers[clf], X_s[X], y, scoring=scoring,\n",
    "                                cv=10, return_train_score=True)\n",
    "        print ('\\t {} - {}'.format(clf, np.mean(scores['test_prec_macro'])))\n",
    "\n",
    "print('Execution time %0.3fs' % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinamento dos hyper-parâmetros\n",
    "Após a execução dos diferentes algoritmos com as diferentes técnicas de extração de feature, chegamos a conclusão de que o algoritmo **SVM Linear** utilizando do **TFIDF** alcançou os melhores resultados. Portanto, escolhemos essa abordagem para a otimização dos hyper-parâmetros.  \n",
    "A melhor forma de escolher um bom hyper-parâmetro é pela tentativa e erro de todas as possíveis combinações de valores para tais parâmetros. Na biblioteca do _Scikit-learn_ podemos encontrar a função **GridSeachCV** e a **RandomSearchCV** que facilitam essa tentativa e erro.   \n",
    "Utilizando do **GridSearch** para um dado modelo, podemos definir um conjunto de parâmetros e valores que queremos testar, então a função _GridSearchCV_ da biblioteca do _Scikit-learn_ cria modelos para todas as combinações possíveis dentro da lista prédefinida de valores dos hyper-parâmetros. Ao final da criação de todos os modelos a melhor combinação é escolhida através da técnica de avaliação _cross-validation_. Com isso podemos notar duas desvantagens:\n",
    "- Computacionalmente caro. Quanto mais parâmetros se quer testar mais caro computacionalmente será. Considereando 5 parâmetros e assuminfo que tenha 5 valores a serem testados para cada parâmetro e um _cross-validation_ de 3 _k-folds_ (padrão da biblioteca) isso resulta em $5^5 = 3125\\ combinações$, $3125 * 3 = 9375\\ fit\\ de\\ modelos$\n",
    "- Esse método por ser tentativa e erro não encontra os parâmetros ideais para seu problema, porém pode encontrar parâmetros quas perfeitos, desde de que inicialmente utilizemos bons \"chutes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 648 candidates, totalling 1944 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 27.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 63.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 113.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 179.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 264.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1944 out of 1944 | elapsed: 291.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 17498.988s\n",
      "\n",
      "Best score: 0.931\n",
      "Best parameters set:\n",
      "\tclf__C: 0.9\n",
      "\tclf__max_iter: 100\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__tol: 0.001\n",
      "\ttfidf__lowercase: False\n",
      "\ttfidf__max_df: 0.6\n",
      "\ttfidf__max_features: None\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('hash', HashingVectorizer()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    #'hash__n_features': (None, 1040000),\n",
    "    #'hash__norm': (None, 'l1', 'l2'),\n",
    "    #'hash__ngram_range': ((1, 1), (1, 2)),\n",
    "    'tfidf__max_features': (None, 1340000),\n",
    "    'tfidf__lowercase': (False, ),\n",
    "    #'tfidf__smooth_idf': (True, False),\n",
    "    #'tfidf__sublinear_tf': (True, False),\n",
    "    'tfidf__max_df': (0.6, 0.7, 0.8),\n",
    "    #'tfidf__min_df': (0.7, 0.8, 1),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),\n",
    "    'tfidf__stop_words': (None, 'english'),\n",
    "    'tfidf__norm': ('l2', ),\n",
    "    'clf__penalty': ('l2', ),\n",
    "    #'clf__dual': (True, False),\n",
    "    'clf__tol': (1e-2, 1e-3, 1e-5),\n",
    "    'clf__C': (0.5, 0.7, 0.9),\n",
    "    #'clf__loss': ('hinge', 'squared_hinge'),\n",
    "    'clf__max_iter': (100, 1000, 2000)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "print(\"done in %0.3fs\\n\" % (time() - t0))\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF time: 18.118s\n",
      "\t SVMLinear with hyperparameter tuning - 0.9419738570273996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['20newgroupsModel.sav']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "vectorizer_tfidf1 = TfidfVectorizer(min_df=1,\n",
    "                                   max_df=0.6, # Retiramos palavras com alta recorrência em documentos, como \"From\" e \"Subject\"\n",
    "                                   ngram_range=(1,2), # N-Gram\n",
    "                                   norm='l2',\n",
    "                                   lowercase=False, # Já transformamos tudo em caixa baixa, isso não é mais necessário\n",
    "                                   stop_words=None, # Para retirar mais possíveis stopwords\n",
    "                                   max_features=None # Retiramos certa de 4000 features\n",
    "                                  )\n",
    "t0 = time()\n",
    "X_tfidf1 = vectorizer_tfidf1.fit_transform(X)\n",
    "print('TF-IDF time: %0.3fs' % (time() - t0))\n",
    "clf_svm1 = LinearSVC(penalty='l2', dual=True, tol=0.001, C=0.9, max_iter=100)\n",
    "\n",
    "score = cross_validate(clf_svm1, X_tfidf1, y, scoring=scoring,\n",
    "                                cv=10, return_train_score=True)\n",
    "print ('\\t SVMLinear with hyperparameter tuning - {}'.format(np.mean(score['test_prec_macro'])))\n",
    "\n",
    "joblib.dump(clf_svm1, '20newgroupsModel.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "Portanto, vimos que para trabalhar com dados textuais a maioria do esforço é empregado no processamento desses dados, para que ao final eles fiquem de forma coerente e númerica para serem utilizados em algoritmos de _machine learning_. Também vimos que utilizando o método de extração de caracteristicas _TF-IDF_ com o classificador _SVM_ com o kernel Linear, obtivemos a melhor predição, com **94%** de acertos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
